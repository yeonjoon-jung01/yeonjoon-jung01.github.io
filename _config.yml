# Content configuration version
version: 2

# Personal info
name: Yeonjoon Jung
title: Undergraduate Student at POSTECH | ML Researcher/Engineer at SqueezeBits
email: yeonjoon.jung@postech.ac.kr

# Dark Mode (true/false/never)
darkmode: false

# Social links
github_username:  yeonjoon-jung01
linkedin_username: yeonjoon-jung

# About Section
about_profile_image: images/profile.jpg
about_content: |
  Hi, my name is <b>Yeonjoon Jung</b>, and I am an undergraduate student at <b>POSTECH</b> 
  majoring in <b>Convergence IT Engineering</b> and <b>Computer Science and Engineering</b>. 
  I am currently taking a leave of absence to complete my mandatory alternative military service 
  as an <b>ML Researcher/Engineer</b> at <b>SqueezeBits</b>, where I focus on <b>optimizing and accelerating AI models</b>.<br><br>

  My recent research interests span the field of <b>Efficient AI</b>â€”including <b>quantization</b>, 
  <b>inference optimization</b>, <b>parameter-efficient fine-tuning (PEFT)</b>-with applications 
  to <b>large language models (LLMs)</b> and <b>diffusion models</b>.<br><br>

  I am always open to <b>collaborations</b> and new research opportunities. 
  Please feel free to <a href="mailto:yeonjoon.jung@postech.ac.kr">contact me</a>.

content:
  - title: News
    layout: text
    content: |
      [11/2025] GraLoRA is now available in the [HuggingFace PEFT Library](https://github.com/huggingface/peft/blob/main/examples/gralora_finetuning/README.md). <br>
      [10/2025] Released [blog post](https://blog.squeezebits.com/77516) on efficient pipeline for diffusion model inference. <br>
      [09/2025] Our [paper](https://arxiv.org/abs/2505.20355) is accepted by NeurIPS 2025 as a <span style="color: red;">Spotlight</span>. <br>
      [06/2025] Released [blog post](https://blog.squeezebits.com/gralora-boosting-fine-tuning-accuracy) on explaining GraLoRA, a novel LoRA fine-tuning method. <br>
      [01/2025] Released [blog post](https://blog.squeezebits.com/vllm-vs-tensorrtllm-13-visionlanguage-models-40761) on exploring Vision Language Model serving. <br>
      [12/2024] Released [blog post](https://blog.squeezebits.com/vllm-vs-tensorrtllm-12-automatic-prefix-caching-38189) on effectiveness of prefix caching. <br>
      [12/2024] Released [blog post](https://blog.squeezebits.com/vllm-vs-tensorrtllm-11-speculative-decoding-37301) on understanding speculative decoding. <br>
      [10/2024] Released [blog post](https://blog.squeezebits.com/vllm-vs-tensorrtllm-2-towards-optimal-batching-for-llm-serving-31349) on analyzing batching in LLM serving. <br>
      [10/2024] Released [blog post](https://blog.squeezebits.com/vllm-vs-tensorrtllm-1-an-overall-evaluation-30703) on evaluating LLM serving with key metrics. <br>
      [11/2023] Our [paper](https://arxiv.org/abs/2312.05611) is accepted by LoG 2023 extended abstract track. <br>
      [08/2023] I joined AI startup, [SqueezeBits](https://squeezebits.com), as an ML Researcher/Engineer. <br>
      [03/2023] I joined [Prof. Ahn's research group](https://sites.google.com/view/sungsooahn0215/home) as an undergraduate researcher. <br>

  - title: Papers
    layout: text
    content: |
      <p style="font-weight: 300; font-size:2.0rem; margin-bottom:3px;"> <b>GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning</b> </p>
      <u>Yeonjoon Jung</u>, Daehyun Ahn, Hyungjun Kim, Taesu Kim, Eunhyeok Park <br>
      <i>Neural Information Processing Systems (NeurIPS) 2025 <span style="color: red;">Spotlight</span></i> <br>
      [ArXiv](https://arxiv.org/abs/2505.20355), [Github](https://github.com/SqueezeBits/GraLoRA) <br><br>

      <p style="font-weight: 300; font-size:2.0rem; margin-bottom:3px;"> <b>Triplet edge attention for algorithmic reasoning</b> </p>
      <u>Yeonjoon Jung</u>, Sungsoo Ahn <br>
      <i>Learning on Graph Conference (LoG), 2023, extended abstract</i> <br>
      [ArXiv](https://arxiv.org/abs/2312.05611) <br>

  - title: Blogs
    layout: text
    content: |
      <p style="font-weight: 300; font-size:2.0rem; margin-bottom:3px;"> <b>Winning both speed and quality: How Yetter deals with diffusion models</b> </p>
      Introducing efficient pipeline for diffusion model inference |
      [Link](https://blog.squeezebits.com/77516) <br><br>

      <p style="font-weight: 300; font-size:2.0rem; margin-bottom:3px;"> <b>GraLoRA: Boosting Fine-Tuning Accuracy Without Extra Cost</b> </p>
      Introducing GraLoRA, a novel LoRA fine-tuning method |
      [Link](https://blog.squeezebits.com/gralora-boosting-fine-tuning-accuracy) <br><br>

      <p style="font-weight: 300; font-size:2.0rem; margin-bottom:3px;"> <b>[vLLM vs TensorRT-LLM] #13. Vision Language Models</b> </p>
      Exploring Vision Language Model serving |
      [Link](https://blog.squeezebits.com/vllm-vs-tensorrtllm-13-visionlanguage-models-40761) <br><br>

      <p style="font-weight: 300; font-size:2.0rem; margin-bottom:3px;"> <b>[vLLM vs TensorRT-LLM] #12. Automatic Prefix Caching</b> </p>
      Effectiveness of prefix caching in LLM serving |
      [Link](https://blog.squeezebits.com/vllm-vs-tensorrtllm-12-automatic-prefix-caching-38189) <br><br>

      <p style="font-weight: 300; font-size:2.0rem; margin-bottom:3px;"> <b>[vLLM vs TensorRT-LLM] #11. Speculative Decoding</b> </p>
      Understanding speculative decoding in LLM serving |
      [Link](https://blog.squeezebits.com/vllm-vs-tensorrtllm-11-speculative-decoding-37301) <br><br>

      <p style="font-weight: 300; font-size:2.0rem; margin-bottom:3px;"> <b>[vLLM vs TensorRT-LLM] #2. Towards Optimal Batching for LLM Serving</b> </p>
      Analyzing batching in LLM serving |
      [Link](https://blog.squeezebits.com/vllm-vs-tensorrtllm-2-towards-optimal-batching-for-llm-serving-31349) or
      [Link2](https://medium.com/squeezebits-team-blog/vllm-vs-tensorrt-llm-2-towards-optimal-batching-for-llm-serving-490b1735c140) <br><br>

      <p style="font-weight: 300; font-size:2.0rem; margin-bottom:3px;"> <b>[vLLM vs TensorRT-LLM] #1. An Overall Evaluation</b> </p>
      Evaluating LLM serving with key metrics |
      [Link](https://blog.squeezebits.com/vllm-vs-tensorrtllm-1-an-overall-evaluation-30703) or
      [Link2](https://medium.com/squeezebits-team-blog/vllm-vs-tensorrt-llm-1-an-overall-evaluation-88f281bf01c7) <br><br>

  - title: Education
    layout: text
    content: |
      <p style="font-weight: 300; font-size:2.0rem; margin-bottom:3px;"> <b>POSTECH</b> </p>
      <i>03/2020 - Present</i> <br>
      <i>Major: Convergence IT Engineering and Computer Science and Engineering</i> <br><br>

      <p style="font-weight: 300; font-size:2.0rem; margin-bottom:3px;"> <b>Korea Science Academy of KAIST</b> </p>
      <i>03/2017 - 02/2020</i> <br>
          
# Build settings
remote_theme: sproogen/resume-theme

sass:
  sass_dir: _sass
  style: compressed

plugins:
 - jekyll-seo-tag
